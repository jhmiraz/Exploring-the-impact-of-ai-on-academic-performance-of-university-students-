# -*- coding: utf-8 -*-
"""random_forest87.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ggm9XguhhJyVlNrKv6YIfVpoOj8r8-rc
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from catboost import CatBoostClassifier
from imblearn.over_sampling import SMOTE

# Load the dataset
df = pd.read_excel('balanced_synthetic_data.xlsx', sheet_name='Sheet1')

# Step 1: Define Target and Features
target_col = '9. Do you think AI has the potential to improve student retention rates over the long run?'
X = df.drop(columns=[target_col])
y = df[target_col]

# Step 2: Encode Target Variable
# Convert target variable to numerical values
y = y.astype('category').cat.codes  # Native pandas encoding for simplicity

# Step 3: Identify Categorical Features
cat_features = [X.columns.get_loc(col) for col in X.select_dtypes(include=['object']).columns]

# Step 4: Handle Class Imbalance with SMOTE
# Convert X to numeric for SMOTE to work
X_encoded = pd.get_dummies(X, drop_first=True)
smote = SMOTE(random_state=42)
X_balanced, y_balanced = smote.fit_resample(X_encoded, y)

# Step 5: Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42)

# Step 6: Re-map Categorical Columns in Training Data
# After SMOTE, recover the categorical feature indices for CatBoost
cat_features_after_smote = [X_encoded.columns.get_loc(col) for col in X.select_dtypes(include=['object']).columns]

# Step 7: Train CatBoost Model with Hyperparameter Tuning
cat_model = CatBoostClassifier(
    iterations=1000,        # Increase iterations for better performance
    learning_rate=0.1,      # Learning rate to avoid overfitting
    depth=8,                # Depth of trees for better feature interactions
    random_seed=42,
    verbose=200,            # Log progress every 200 iterations
    cat_features=cat_features_after_smote
)

cat_model.fit(X_train, y_train)

# Step 8: Evaluate Model
y_pred = cat_model.predict(X_test)
cat_accuracy = accuracy_score(y_test, y_pred)

# Print Results
print(f"CatBoost Accuracy: {cat_accuracy * 100:.2f}%")
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Step 9: Feature Importance
feature_importances = pd.DataFrame({
    'Feature': X_encoded.columns,
    'Importance': cat_model.feature_importances_
}).sort_values(by='Importance', ascending=False)

print("\nTop Features by Importance:")
print(feature_importances.head(10))

"""phase2:

**question 9:**
"""

import pandas as pd
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# Load the raw data (assuming it's in an XLSX file)
df = pd.read_excel('balanced_synthetic_data.xlsx', sheet_name='Sheet1')

# Step 1: Handle Missing Values (if not already done)
for col in df.columns:
    if df[col].isnull().sum() > 0:
        most_frequent_value = df[col].mode()[0]
        df[col] = df[col].fillna(most_frequent_value)

# Step 2: Separate Features and Target
target_col = '9. Do you think AI has the potential to improve student retention rates over the long run?'
X = df.drop(columns=[target_col])
y = df[target_col]

# Step 3: Apply SMOTE to handle class imbalance
smote = SMOTE(random_state=42)
X_balanced, y_balanced = smote.fit_resample(X, y)

# Step 4: Feature Scaling using StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_balanced)

# Step 5: Split the new synthetic data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_balanced, test_size=0.3, random_state=42)

# Step 6: Train a Random Forest Classifier
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train, y_train)

# Step 7: Make Predictions
y_pred = rf_model.predict(X_test)

# Step 8: Evaluate the Model
accuracy = accuracy_score(y_test, y_pred)
print(f"Random Forest Accuracy: {accuracy * 100:.2f}%")

print("\nClassification Report:")
print(classification_report(y_test, y_pred))

import pandas as pd
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# Load the raw data (assuming it's in an XLSX file)
df = pd.read_excel('balanced_synthetic_data.xlsx', sheet_name='Sheet1')

# Step 1: Handle Missing Values
for col in df.columns:
    if df[col].isnull().sum() > 0:
        most_frequent_value = df[col].mode()[0]
        df[col] = df[col].fillna(most_frequent_value)

# Step 2: Encode categorical features
label_encoders = {}
for col in df.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le

# Define a function to test a specific variable

def evaluate_variable(target_col):
    print(f"\nEvaluating for target variable: {target_col}\n")

    # Step 3: Separate Features and Target
    X = df.drop(columns=[target_col])
    y = df[target_col]

    # Step 4: Apply SMOTE to handle class imbalance
    smote = SMOTE(random_state=42)
    X_balanced, y_balanced = smote.fit_resample(X, y)

    # Step 5: Feature Scaling using StandardScaler
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_balanced)

    # Step 6: Split the data into train and test sets
    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_balanced, test_size=0.3, random_state=42)

    # Step 7: Train a Random Forest Classifier
    rf_model = RandomForestClassifier(random_state=42)
    rf_model.fit(X_train, y_train)

    # Step 8: Make Predictions
    y_pred = rf_model.predict(X_test)

    # Step 9: Evaluate the Model
    accuracy = accuracy_score(y_test, y_pred)
    print(f"Random Forest Accuracy for {target_col}: {accuracy * 100:.2f}%")

    print("\nClassification Report:")
    print(classification_report(y_test, y_pred))

# List of target variables to evaluate
target_variables = [
    '7. Do you think AI tools could be used effectively to support mental health and well-being in education?',
    '11. Do you think AI will have a sustained positive impact on student performance over time?',
    '13. Do you think AI tools make students more productive in their studies?'
]

# Evaluate each variable
for target in target_variables:
    evaluate_variable(target)

import pandas as pd
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Load the raw data (assuming it's in an XLSX file)
df = pd.read_excel('balanced_synthetic_data.xlsx', sheet_name='Sheet1')

# Step 1: Handle Missing Values
for col in df.columns:
    if df[col].isnull().sum() > 0:
        most_frequent_value = df[col].mode()[0]
        df[col] = df[col].fillna(most_frequent_value)

# Step 2: Encode categorical features
label_encoders = {}
for col in df.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le

# Function to visualize confusion matrix
def plot_confusion_matrix(y_test, y_pred, target_col):
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])
    plt.title(f"Confusion Matrix for {target_col}")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.show()

# Function to visualize feature importance
def plot_feature_importance(model, X, target_col):
    feature_importances = pd.Series(model.feature_importances_, index=X.columns)
    feature_importances.sort_values(ascending=False, inplace=True)
    plt.figure(figsize=(10, 6))
    sns.barplot(x=feature_importances, y=feature_importances.index, palette="viridis")
    plt.title(f"Feature Importance for {target_col}")
    plt.xlabel("Importance Score")
    plt.ylabel("Features")
    plt.tight_layout()
    plt.show()

# Function to evaluate a specific variable
def evaluate_variable(target_col):
    print(f"\nEvaluating for target variable: {target_col}\n")

    # Step 3: Separate Features and Target
    X = df.drop(columns=[target_col])
    y = df[target_col]

    # Step 4: Apply SMOTE to handle class imbalance
    smote = SMOTE(random_state=42)
    X_balanced, y_balanced = smote.fit_resample(X, y)

    # Step 5: Feature Scaling using StandardScaler
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_balanced)

    # Step 6: Split the data into train and test sets
    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_balanced, test_size=0.3, random_state=42)

    # Step 7: Train a Random Forest Classifier
    rf_model = RandomForestClassifier(random_state=42)
    rf_model.fit(X_train, y_train)

    # Step 8: Make Predictions
    y_pred = rf_model.predict(X_test)

    # Step 9: Evaluate the Model
    accuracy = accuracy_score(y_test, y_pred)
    print(f"Random Forest Accuracy for {target_col}: {accuracy * 100:.2f}%")

    print("\nClassification Report:")
    print(classification_report(y_test, y_pred))

    # Step 10: Visualize Results
    plot_confusion_matrix(y_test, y_pred, target_col)
    plot_feature_importance(rf_model, pd.DataFrame(X_balanced, columns=X.columns), target_col)

# List of target variables to evaluate
target_variables = [
    '7. Do you think AI tools could be used effectively to support mental health and well-being in education?',
    '11. Do you think AI will have a sustained positive impact on student performance over time?',
    '13. Do you think AI tools make students more productive in their studies?'
]

# Evaluate each variable
for target in target_variables:
    evaluate_variable(target)

"""feature analysis insight **bold text**"""

import pandas as pd
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
import matplotlib.pyplot as plt
import numpy as np

# Load the raw data
df = pd.read_excel('balanced_synthetic_data.xlsx', sheet_name='Sheet1')

# Handle Missing Values
for col in df.columns:
    if df[col].isnull().sum() > 0:
        most_frequent_value = df[col].mode()[0]
        df[col] = df[col].fillna(most_frequent_value)

# Encode categorical features
label_encoders = {}
for col in df.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le

# Function to evaluate variable
def evaluate_variable(target_col):
    print(f"\nEvaluating for target variable: {target_col}\n")

    # Separate Features and Target
    X = df.drop(columns=[target_col])
    y = df[target_col]

    # Apply SMOTE to handle class imbalance
    smote = SMOTE(random_state=42)
    X_balanced, y_balanced = smote.fit_resample(X, y)

    # Feature Scaling
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_balanced)

    # Split data into train and test sets
    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_balanced, test_size=0.3, random_state=42)

    # Train a Random Forest Classifier
    rf_model = RandomForestClassifier(random_state=42)
    rf_model.fit(X_train, y_train)

    # Make Predictions
    y_pred = rf_model.predict(X_test)

    # Evaluate the Model
    accuracy = accuracy_score(y_test, y_pred)
    print(f"Random Forest Accuracy for {target_col}: {accuracy * 100:.2f}%")

    print("\nClassification Report:")
    print(classification_report(y_test, y_pred))

    # Feature Importance Analysis
    feature_importance_analysis(rf_model, X.columns, target_col)

    # Generate Recommendations
    important_features = [X.columns[i] for i in np.argsort(rf_model.feature_importances_)[::-1]]
    generate_recommendations(target_col, important_features)

# Feature Importance Analysis
def feature_importance_analysis(model, feature_names, target_col):
    importances = model.feature_importances_
    indices = np.argsort(importances)[::-1]

    print(f"\nFeature Importance for Target Variable: {target_col}")
    for i in range(len(feature_names)):
        print(f"{i + 1}. {feature_names[indices[i]]} ({importances[indices[i]]:.4f})")

    plt.figure(figsize=(10, 6))
    plt.title(f"Feature Importance for {target_col}")
    plt.bar(range(len(feature_names)), importances[indices], align="center")
    plt.xticks(range(len(feature_names)), [summarize_variable(feature_names[i]) for i in indices], rotation=90)
    plt.xlabel("Features")
    plt.ylabel("Importance Score")
    plt.tight_layout()
    plt.show()

def summarize_variable(variable_name):
    keywords = {
        "support mental health and well-being in education?": "support mental health and well-being in education?",
        "student performance": "positive impact on performance",
        "more productive": "improve productivity",
        "retention rates": "improve retention rates",
    }
    for key, summary in keywords.items():
        if key in variable_name.lower():
            return summary
    return variable_name

# Generate Recommendations
def generate_recommendations(target_col, important_features):
    print(f"\nRecommendations for Improving {target_col}:")
    for feature in important_features[:3]:  # Top 3 features
        if feature == "Study Hours":
            print("- Encourage students to adopt consistent study schedules.")
        elif feature == "Use of AI Tools":
            print("- Provide training on how to effectively use AI tools.")
        elif feature == "Group Discussions":
            print("- Promote group study sessions to enhance learning.")
        else:
            print(f"- Focus on improving {feature} as it significantly impacts {target_col}.")

# List of target variables to evaluate
target_variables = [
    '7. Do you think AI tools could be used effectively to support mental health and well-being in education?',
    '11. Do you think AI will have a sustained positive impact on student performance over time?',
    '13. Do you think AI tools make students more productive in their studies?'
]

# Evaluate each target variable
for target in target_variables:
    evaluate_variable(target)

import pandas as pd

# Reading an excel file
df = pd.read_excel('balanced_synthetic_data.xlsx') # Changed pd.read to pd.read_excel

# Print the column headers
print(df.columns)